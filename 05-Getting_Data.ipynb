{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class='bar_title'></div>\n",
    "\n",
    "*Introduction to Data Science (IDS)*\n",
    "\n",
    "# Data Acquisition\n",
    "\n",
    "Gunther Gust <br>\n",
    "Chair for Enterprise AI<br>\n",
    "Data Driven Decisions (D3) Group<br>\n",
    "Center for Artificial Intelligence and Data Science (CAIDAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/d3.png\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/CAIDASlogo.png\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sources\n",
    "This lecture relies on:\n",
    "- https://github.com/kwaldenphd/apis-python\n",
    "- https://github.com/oxylabs/Python-Web-Scraping-Tutorial/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/ao_data_acquisition.png\" style=\"width:100%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Obtaining Data\n",
    "Before diving into APIs and web scraping, it’s important to understand the different ways data scientists commonly obtain data. Here’s an overview of the primary methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Open Datasets\n",
    "Many organizations, universities, and governments make datasets publicly available online. These open datasets are usually provided in downloadable formats, such as CSV, Excel, or JSON, and are often already cleaned and well-documented (hopefully).\n",
    "\n",
    "Examples:\n",
    "- [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "- Github repos often provide datasets\n",
    "- [Data.gov](https://data.gov/)\n",
    "- [UCI Machine Learning Repo](https://archive.ics.uci.edu/)\n",
    "- specifically for Würzburg: https://opendata.wuerzburg.de/pages/wue-dashboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### APIs\n",
    "APIs provide a __structured and standardized way__ to retrieve data from an application or service. Companies and organizations often make APIs available to allow developers to access data in real-time, with data often delivered in formats like __JSON or XML.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Examples:\n",
    "- Twitter API\n",
    "- Weather API\n",
    "- Nasa API\n",
    "- Spotify API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Such APIs often provide you with (near-) real time data. Access may be restricted or __rate limits might apply__ in some cases. Handling APIs will be the first part of this lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Web Scraping\n",
    "Web scraping involves extracting data __directly from websites__ by __parsing their HTML__ content. This technique is useful when there’s no API or dataset available, but the data is accessible on a website.\n",
    "\n",
    "We will look into this topic in a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Collection via Surveys and Experiments\n",
    "\n",
    "In cases where no existing data sources are available, researchers may conduct surveys or experiments to generate their own data. This method is commonly used in fields like social science and psychology, but it can also be of high relevance in computer sciences, where data is collected from sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general, if no open dataset is available for your usecase and you can't conduct an experiment due to the nature of the data you are looking for or __time constraints,__ the two most advantageous ways to obtain data are __APIs and Web Scraping.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## APIs\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/api.jpg\" style=\"width:40%;\" />\n",
    "\n",
    "\n",
    "An **API** (Application Programming Interface) is a __set of rules and protocols__ that allows one application to interact with another. APIs let developers access data and services from other applications without knowing how they're implemented.\n",
    "\n",
    "If an API is available, using it is often __preferred over Web Scraping__ since they provide structured data that is often more accessible than scraping HTML from web pages,  are often faster and more reliable than web scraping and help ensure that you are acessing legal and up-to-date data (more on this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Making an API call\n",
    "We use the requests module to send __HTTP requests__ (i.e. request data via the world wide web) using Python. These requests use a range of methods that let us interact with elements of the API:\n",
    "\n",
    "- **delete(url, args)**\tsends as DELETE request to the specified URL\n",
    "- **get(url, params, args)** sends a GET request to the specified URL\n",
    "- **head(url, args)** sends a HEAD request to the specified URL\n",
    "- **patch(url, data, args)** sends a PATCH request to the specified URL\n",
    "- **post(url, data, json, args)** sends a POST request to the specified URL\n",
    "- **put(url, data, args)** sends a PUT request to the specified URL\n",
    "- **request(method, url, args)** sends a request of the specified method to the specified URL\n",
    "\n",
    "The HTTP request returns a __response object__ that includes whatever __data__ is returned as part of the API call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try to call the [GitHub API](https://docs.github.com/en/rest?) and search for Python repositories sorted by stars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# store API url\n",
    "url = 'https://api.github.com/search/repositories?q=language:python&sort=stars'\n",
    "\n",
    "# assign the headers- not always necessary, but something we have to do with the GitHub API\n",
    "headers = {'Accept': 'application/vnd.github.v3+json'}\n",
    "\n",
    "# assign the requests method\n",
    "r = requests.get(url, headers=headers)\n",
    "\n",
    "# print a status update for the requests command\n",
    "print(f\"Status code: {r.status_code}\")\n",
    "\n",
    "# store API response to variable\n",
    "response_dict = r.json()\n",
    "\n",
    "# process results\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's go through the individual elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### URL\n",
    "\n",
    "Here's a breakdown of the URL:\n",
    "\n",
    "- Base URL: `https://api.github.com/search/repositories` This is the endpoint for searching repositories on GitHub.\n",
    "- Query Parameters: \n",
    "    - `q=language:python:` This filters the search to repositories where the primary language is Python.\n",
    "    - `sort=stars:` This sorts the repositories by the number of stars, showing the most popular ones first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Header of an HTTP Request\n",
    "\n",
    "A **header of an HTTP request** is a key part of the request that provides metadata about the request being sent to the server. It is a collection of key-value pairs that convey additional information such as how to handle the request, client capabilities, and preferences.\n",
    "\n",
    "\n",
    "HTTP request headers are part of the request message and are sent **after the request line** (e.g., `GET /index.html HTTP/1.1`) and **before the request body** (if any). They are structured as key-value pairs:\n",
    "\n",
    "     - `Host`: Specifies the domain name of the server (required in HTTP/1.1).\n",
    "     - `User-Agent`: Provides information about the client (e.g., browser, device).\n",
    "     - `Accept`: Informs the server about the media types the client can process.\n",
    "     - `Authorization`: Contains credentials for authentication.\n",
    "     - `Cookie`: Sends cookies to the server.\n",
    "     - `Content-Type`: Indicates the MIME type of the body content.\n",
    "     - `Content-Length`: Specifies the size of the request body in bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Responses\n",
    "\n",
    "When you interact with web servers (e.g., via APIs or web scraping), servers __respond with HTTP status codes__ that indicate the result of your request. Here are some common codes that can help you debug and manager your access:\n",
    "\n",
    "- 200 OK: The request was successful, and the server returned the data.\n",
    "- 201 Created: The request was successful, and something new was created (e.g., a new record).\n",
    "- 400 Bad Request: The request was invalid, often due to missing or incorrect parameters.\n",
    "- 401 Unauthorized: Authentication is required, and the provided credentials are missing or invalid.\n",
    "- 403 Forbidden: The request was understood, but the server refuses to fulfill it (often due to permissions).\n",
    "- 404 Not Found: The server couldn’t find the requested resource (e.g., a non-existent endpoint).\n",
    "- 500 Internal Server Error: An error occurred on the server, unrelated to the request itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can start to explore the data returned by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total repositories: {response_dict['total_count']}\")\n",
    "\n",
    "repo_dicts = response_dict['items']\n",
    "print(f\"Repositories returned: {len(repo_dicts)}\")\n",
    "\n",
    "repo_dict = repo_dicts[0]\n",
    "print(\"\\nSelected information about first repository:\")\n",
    "print(f\"Name: {repo_dict['name']}\")\n",
    "print(f\"Owner: {repo_dict['owner']['login']}\")\n",
    "print(f\"Stars: {repo_dict['stargazers_count']}\")\n",
    "print(f\"Repository URL: {repo_dict['html_url']}\")\n",
    "print(f\"Created: {repo_dict['created_at']}\")\n",
    "print(f\"Updated: {repo_dict['updated_at']}\")\n",
    "print(f\"Description: {repo_dict['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is a github repo with an extensive collection of public APIs you can try out:\n",
    "\n",
    "https://github.com/public-apis/public-apis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most APIs, you will have to provide some credentials when calling the `requests.get()` function. This would be done via\n",
    "`requests.get(url, auth=('user', 'password'))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1\n",
    "What is wrong in this API request? Correct the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_url = \"https://official-joke-api.appspot.com/joke/random\"\n",
    "response = requests.get(joke_url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Webscraping\n",
    "\n",
    "Web scraping is a technique to automatically extract data from websites. Unlike APIs, which provide structured data, web scraping involves fetching and parsing HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This will go to the Wikipedia page for the web scraping and print the first paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://en.wikipedia.org/wiki/Web_scraping\")\n",
    "bs = BeautifulSoup(response.text,\"lxml\")\n",
    "print(bs.find(\"p\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Components of a Web Scraping with Python Code\n",
    "\n",
    "The main building blocks for any web scraping code is like this:\n",
    "\n",
    "1. Get HTML\n",
    "2. Parse HTML into Python object\n",
    "3. Save the data extracted\n",
    "\n",
    "In most cases, there is no need to use a browser to get the HTML. While HTML contains the data, the other files that the browser loads, like images, CSS, JavaScript, etc., just make the website pretty and functional. Web scraping is focused on data. Thus in most cases, there is no need to get these helper files.\n",
    "\n",
    "There will be some cases when you do need to open the browser. Python makes that easy too. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Web scraping with Python is easy due to the many useful libraries available:\n",
    "- The [Requests](https://docs.python-requests.org/en/master/) library is used to get the HTML files, bypassing the need to use a browser. We already saw that one in the API call.\n",
    "- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) is used to convert the raw HTML into a Python object, also called parsing. We will be working with Version 4 of this library, also know as `bs4` or `BeautifulSoup4`.\n",
    "- The [CSV](https://docs.python.org/3/library/csv.html) library is part of the standard Python installation. No separate installation is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "url_to_parse = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
    "response = requests.get(url_to_parse)\n",
    "print(response)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This messy string is the HTML code that the website is made of. All the content, its position and formatting is specified here. In order to convert this string into something that can be queried to find the specific information we will use `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### BeautifulSoup\n",
    "Beautiful Soup provides simple methods for __navigating, searching, and modifying the HTML__ (check out the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) for more usage examples). It takes care of encoding by automatically converting into UTF-8.\n",
    "The first step is to decide the parser that you want to use. Usually, `lxml` is the most commonly used.  This will need a separate install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "print(soup.prettify()) #returns the document as a well-formatted, readable string with proper indentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can then use the following __syntax__ to access html tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title)\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Similarly `soup.h1` will return the **first** `h1` tag it finds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `find()` and `find_all()`\n",
    "\n",
    "Perhaps the most commonly used methods are `find()` and `find_all()`. \n",
    "\n",
    "The signature of find looks something like this:\n",
    "\n",
    "find(name=None, attrs={}, recursive=True, text=None, **kwargs)\n",
    "\n",
    "In order to understand how you can search for a certain element in a webpage, you can go to the webpage and click on `Inspect` to open the HTML view.\n",
    "Once you idientified the information that you are looking for, the find method can be used to find elements based on `name`, `attributes`, or `text`. This should cover most of the scenarios. For scenarios like finding by `class`, there is `**kwargs` that can take other filters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    " #### Example \n",
    " \n",
    "Let’s open the [Wikipedia Python page](https://en.wikipedia.org/wiki/Python_(programming_language)) and get the __table of contents.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Moving on to Wikipedia example, the first step is to look at the __HTML markup for the table of contents__ to be extracted. \n",
    "\n",
    "Right-click on the part (so called division or `div`) that contains the table of contents and examine its markup. It is clear that the whole table of contents is in a div tag with the class attribute set to toc:\n",
    "```html\n",
    "<div id=\"vector-toc\" class=\"vector-toc vector-pinnable-element\">\n",
    "```\n",
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/scraping_python.png\" style=\"width:80%;\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we simply run `soup.find(\"div\")`, it will return the first div it finds - similar to writing `soup.div`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = soup.find(\"div\")\n",
    "print(toc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Find & filter\n",
    "\n",
    "If we simply run `soup.find(\"div\")`, it will return the first div it finds - similar to writing `soup.div`. Finding the Table of Contents needs __filtering as we need a specific div.__ We are lucky in this case as it has an `id `attribute. The following line of code can extract the div element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "toc = soup.find(\"div\",id=\"vector-toc\")\n",
    "print(toc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Further details\n",
    "\n",
    "- Note the second parameter here - `id=\"vector-toc\"`.  The find method does not have a named parameter `id`, but still this works because of the implementation of the filter using the `**kwargs`. (`**kwargs` is a way to pass a variable number of keyword arguments to a function. It allows you to accept any number of named arguments in a function call. The kwargs stands for \"keyword arguments,\" and the ** syntax is what makes it special. See e.g. [this example](https://www.geeksforgeeks.org/args-kwargs-python/))\n",
    "\n",
    "- Be careful with CSS class though. `class `is a reserved keyword in Python. It cannot be used as a parameter name directly.  There are two workarounds – first, just use `class_` instead of `class`. The second workaround is to use a dictionary as the second argument.\n",
    "\n",
    "- This means that the following two statements are same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc2 = soup.find(\"div\",class_=\"vector-toc vector-pinnable-element\")\n",
    "toc3 = soup.find(\"div\",{\"class\": \"vector-toc vector-pinnable-element\"}) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Filtering based on several criteria\n",
    "\n",
    "The advantage of using a __dictionary__ is that __more than one attribute__ can be specified. For example,if you need to specify both class and id, you can use the find method in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\",{\"class\": \"vector-toc vector-pinnable-element\", \"id\":\"vector-toc\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `find_all()` to find multiple elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Consider this scenario - the object is to create a CSV file, which has two columns. The first column contains the __heading number__ and the second column contains the __heading text.__\n",
    "\n",
    "To find multiple columns, we can use the `find_all` method.\n",
    "\n",
    "This method works the __same way as the `find()` method__ works, just that instead of one element, it returns a list of all the elements that match criteria. If we look at the source code, we can see that all the heading text is inside a span, with toctext as class. We can use find_all method to extract all these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "toc_elements = soup.find_all(\"div\",class_=\"vector-toc-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This returned all divs with class `vector-toc-text`. Each of them consists of two spans, the first one indicating the number of the element in the ToC, the second one containing the title. Now if we want to properly work with that, a dictionary of this information would be more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "toc_data= []\n",
    "for element in toc_elements:\n",
    "    if element.find(\"span\", class_=\"vector-toc-numb\"):\n",
    "        heading_number = element.find(\"span\", class_=\"vector-toc-numb\").text\n",
    "        heading_text = element.find_all(\"span\")[-1].text\n",
    "        toc_data.append({\n",
    "            'heading_number' : heading_number,\n",
    "            'heading_text' : heading_text,\n",
    "        })\n",
    "\n",
    "toc_data = pd.DataFrame(toc_data)\n",
    "toc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now you could export the scraped information as a csv file in order to work with it in another project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_data.to_csv('toc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "Go to [Quotes to Scrape](http://quotes.toscrape.com/) and use the browser's Inspect tool.\n",
    "Notice that each quote is inside a \n",
    "```html\n",
    "<span class=\"text\">\n",
    "```\n",
    "element. We’ll target only these elements.\n",
    "\n",
    "Scrape all the quotes displayed on this page and print them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common Data Formats\n",
    "As you have already seen in the examples, a common format returned by APIs is JSON.\n",
    "\n",
    "When you interact with APIs or scrape websites, you'll often encounter different data formats. Here are the main ones:\n",
    "\n",
    "- JSON (JavaScript Object Notation): Commonly used with APIs. We already encountered this usig the GitHub API.\n",
    "- HTML: Used in web pages and requires parsing to extract data.\n",
    "- XML: Sometimes used by older APIs or services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### JSON (JavaScript Object Notation)\n",
    "JSON is a lightweight data format that is widely used in web APIs for exchanging data between a server and a client. It’s easy for both humans and machines to read and write.It is organized in a key-value structure, similar to Python dictionaries, making it easy to work with in Python. It can contain nested objects and arrays.\n",
    "\n",
    "The example.json file contains a short example on how a JSON file might look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HTML (HyperText Markup Language)\n",
    "HTML is the standard markup language for creating web pages. When we scrape websites, we usually work with HTML documents to extract data displayed on the page, such as text, images, links, and tables. It uses tags to define elements (e.g., `<div>, <p>, <span>, <table>`). Elements are often nested, forming a tree structure (DOM - Document Object Model) that represents the layout of the webpage.\n",
    "\n",
    "We already encountered this data type in the scraping example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### XML (eXtensible Markup Language)\n",
    "XML is another markup language like HTML but is designed to store and transport data. It’s commonly used in data interchange between systems, especially in older APIs or specific industries (e.g., banking, healthcare).\n",
    "\n",
    "It uses nested tags to define data, similar to HTML, but it’s more flexible as developers define their own tags based on the type of data. It often has a hierarchical structure, making it useful for representing complex, nested data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_content = \"\"\"\n",
    "<course>\n",
    "    <name>Data Science</name>\n",
    "    <module>\n",
    "        <title>Introduction to Python</title>\n",
    "        <duration>2 weeks</duration>\n",
    "    </module>\n",
    "    <module>\n",
    "        <title>Data Analysis with Pandas</title>\n",
    "        <duration>3 weeks</duration>\n",
    "    </module>\n",
    "</course>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "XML can be handled in Python via the xml.etree.ElementTree module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ET.fromstring(xml_content)\n",
    "\n",
    "for module in root.findall('module'):\n",
    "    title = module.find('title').text\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outlook: Advanced Data Collection Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Selenium: Working with Dynamic Websites\n",
    "\n",
    "Selenium is especially useful for automating interactions on websites that rely heavily on JavaScript (programming language that enables __interactive web pages and dynamic user experiences__). It allows for actions such as:\n",
    "\n",
    "- Clicking buttons\n",
    "- Filling out forms\n",
    "- Scrolling through pages\n",
    "- Taking screenshots\n",
    "- Running custom JavaScript\n",
    "\n",
    "$\\Rightarrow$ Great option for scraping data from dynamic websites. Unlike traditional tools that only retrieve raw HTML and JavaScript code, Selenium can simulate human interaction, enabling access to the underlying data on these complex pages.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/selenium_interacting.png\" style=\"width:20%; float:left;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in using Selenium for your projects, [this video](https://www.youtube.com/watch?v=nOV-UrRU9N4) is a good ressource to start with the setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Automating Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automating data collection can be useful for ensuring that we gather data consistently and without needing manual intervention. For example, we might want to:\n",
    "- Collect data from an API every day at a certain time.\n",
    "- Capture changes on a website at regular intervals.\n",
    "- Aggregate data over time for long-term analysis.\n",
    "\n",
    "Two tools for scheduling such tasks are:\n",
    "- **Cron**: A time-based job scheduler commonly used in Unix-like operating systems (Linux and macOS).\n",
    "- **Task Scheduler**: A Windows utility for automating tasks by scheduling programs or scripts to run at specific times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Cron**\n",
    "\n",
    "Here, the Syntax looks like this:\n",
    "```{cron}\n",
    "* * * * * command\n",
    "```\n",
    "so for example for a script called `scrape_data.py` that is supposed to be executed every day at midnight:\n",
    "```{cron}\n",
    "0 0 * * * /usr/bin/python3/path/scrape_data.py\n",
    "```\n",
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/cron_syntax.png\" style=\"width:50%; float:left;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Task Scheduler**\n",
    "\n",
    "Example Setup:\n",
    "1. Open Task Scheduler and create a basic task.\n",
    "2. Trigger: Daily at 1:00 AM.\n",
    "3. Action: Run python with the path to your script\n",
    "\n",
    "See e.g. [this post](https://www.jcchouinard.com/python-automation-using-task-scheduler/) for a more detailed description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ethical Considerations\n",
    "\n",
    "Web scraping gives the scraper a lot of power, especially when it comes to websites that handle a lot of user data and contain personal information. Without setting up ethical standards and a moral code for web scraping, it can be hard to differentiate between sleazy web scrapers looking to profit from their data at the expense of others, and those who wish to innovate and learn new things using the data available online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Key considerations to take into account when you scrape a website:\n",
    "\n",
    "1. If there is an __API available__, use it.\n",
    "\n",
    "2. Respect __Robots.txt:__ Websites use a robots.txt file to communicate their scraping policies. This file specifies which pages or sections of a website can be crawled and scraped by bots. Always check and respect these guidelines, as ignoring them may violate the website's terms of service.\n",
    "\n",
    "3. Abide by __Terms of Service:__ Many websites have terms of service (ToS) that explicitly prohibit or restrict web scraping. Violating these terms could lead to legal consequences, including being banned from the site or facing litigation. It's important to read and understand a site's ToS before scraping.\n",
    "\n",
    "4. Avoid __Overloading Servers:__ Sending too many requests in a short period can overwhelm a website’s server. Use throttling, rate limiting, or pauses between requests to reduce server load and avoid causing disruptions.\n",
    "\n",
    "5. Respect __Copyright and Intellectual Property:__ Scraping copyrighted content (such as articles, images, or databases) without permission could be a violation of intellectual property laws. Always ensure you are scraping publicly available data or data that you have explicit permission to use.\n",
    "\n",
    "6. __Data Privacy:__ Avoid scraping personal or sensitive information.\n",
    "\n",
    "7. ___Transparency and Fair Use:__ When scraping, be transparent about your intentions if asked, and ensure that your use of the scraped data aligns with fair use principles.\n",
    "\n",
    "Consider [this blog post](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01) on ethical web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mentimeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Next lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/vhaus63/ids_data/main/ao_data_prep.png\" style=\"width:100%; float:left;\" />"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "rise": {
   "custom_css": "./rise.css",
   "enable_chalkboard": false,
   "overlay": "<div class='background'></div><div class='header'>Introduction to Data Science (IDS)</div><div class='logo'><img src='images/d3logo.png'></div><div class='bar'></div>",
   "scroll": true,
   "slideNumber": true,
   "start_slideshow_at": "selected"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
